# ğŸ§  Aurora Member Q&A â€” RAG Architecture & Design Overview

While developing the **Aurora Member Q&A System**, I designed a modular pipeline based on a **Retrieval-Augmented Generation (RAG)** architecture.  
The system answers natural-language questions about members by analyzing their historical messages â€” with a strong focus on **accuracy, transparency, and deployability**.

---

## ğŸš€ Overview of the RAG Pipeline

The backend consists of three main stages: **Retrieval**, **Augmentation**, and **Generation** â€” orchestrated through a FastAPI service.

### **1ï¸âƒ£ Retrieval**
When a user asks a question, I first identify the referenced member using a **hybrid literal + fuzzy name-matching** algorithm.  
Once identified, I perform **semantic retrieval** from a **ChromaDB** vector store that holds pre-computed embeddings of all messages.

**Key retrieval logic:**
- Message embeddings are generated using **`sentence-transformers/all-mpnet-base-v2`**.  
- A persistent ChromaDB index ensures fast re-use between sessions.  
- Retrieval first searches within the detected memberâ€™s messages, then falls back to global search.  
- I apply **centroid expansion** to enrich context and ensure topical coherence.  

> ğŸ§© *Result:* A ranked list of the most semantically relevant and recent messages for that member.

---

### **2ï¸âƒ£ Augmentation**
The retrieved messages are cleaned, sorted, and formatted into readable, time-stamped context snippets.  
This context is then appended to the question before sending it to the LLM.
	â€¢	Builds a deterministic prompt enforcing factual reasoning hierarchy.
	â€¢	Interfaces with the OpenAI API (via OPENAI_API_KEY).
	â€¢	Returns structured, grounded answers while normalizing fallbacks for consistency.

---

### **3ï¸âƒ£ Generation**
For generation, I use **OpenAI GPT (`gpt-4o-mini`)** with a structured reasoning hierarchy to ensure factual consistency.

**Response logic hierarchy:**
1. âœ… *If the answer is explicitly found*, return a concise factual answer.  
2. âš™ï¸ *If it can be inferred*, start with  
   > â€œI donâ€™t have the exact information for this, but based on the available contextâ€¦â€  
3. ğŸš« *If no context applies*, respond with  
   > â€œI donâ€™t have any information about the question you asked.â€

This approach prevents hallucinations and maintains concise, trustworthy responses.

---

## âš™ï¸ System Components

### ğŸ§© **FastAPI Application (`main.py`)**
- `/ask` endpoint handles incoming questions and orchestrates the RAG process.  
- On startup, loads messages via the public API (`utils.py`) and checks for a Chroma index.  
- If missing, it rebuilds embeddings automatically using `build_index()`.

### ğŸ§© **Retriever Module (`retriever.py`)**
- Generates embeddings with **SentenceTransformer**.  
- Uses **ChromaDB** with cosine similarity for semantic search.  
- Detects users using hybrid **literal + fuzzy matching** (RapidFuzz).  
- Implements multi-stage retrieval:
  - User-scoped search  
  - Centroid expansion  
  - Deduplication and recency sorting  

### ğŸ§© **LLM Module (`llm.py`)**
- Builds contextual prompts with time-stamped conversation snippets.  
- Uses **OpenAI GPT** for controlled reasoning.  
- Enforces strict generation rules to avoid hallucinations.  
- Handles fallback responses gracefully if API errors occur.

---

## ğŸ§® Alternative Approaches Considered

| Component | Alternative | Problem Encountered | Final Decision |
|------------|-------------|--------------------|----------------|
| **Knowledge Base** | SQLite / PostgreSQL full-text search | Poor semantic recall. | âœ… Switched to ChromaDB |
| **Embedding Model** | `MiniLM-L6-v2` (faster) | Missed nuanced context. | âœ… Chose `all-mpnet-base-v2` |
| **User Detection** | Pure fuzzy match | Incorrect user attribution. | âœ… Hybrid literal + fuzzy |
| **Retrieval Scope** | Global retrieval | Pulled irrelevant context. | âœ… User-scoped with fallback |
| **Generation** | Free-form LLM output | Hallucinated information. | âœ… Controlled rule-based output |
| **Index Handling** | Rebuild each run | High startup latency. | âœ… Cached persistent Chroma index |

---

## ğŸ§© Problems Faced & Fixes

| Issue | Root Cause | Solution |
|-------|-------------|----------|
| âŒ Incorrect user mapping | Fuzzy match confusion between similar names | Combined literal substring + fuzzy threshold |
| ğŸ•’ Cold-start latency (~30 s) | SentenceTransformer model loading on CPU | Warm-up during FastAPI startup |
| âš ï¸ Duplicate / blank API messages | Data noise from source | Normalized text & filtered whitespace |
| ğŸ”Œ 502 Bad Gateway on Railway | Fixed port in frontend | Used `npx serve -s dist -l $PORT` |
| ğŸ§± Host blocking on Vite preview | Railway domain not allowed | Added `preview.allowedHosts` in `vite.config.js` |
| ğŸ’¬ OpenAI network / quota errors | API interruptions | Implemented exception handling + fallbacks |

---

## ğŸ” Key Takeaways

- The **RAG pipeline** significantly improved factual accuracy and transparency.  
- **Quality embeddings** matter more than small latency gains.  
- **Persistent Chroma storage** removed the need for re-indexing.  
- A **structured prompt hierarchy** ensured safe and interpretable generation.  
- Clear module separation simplified maintenance and debugging.  

---

## ğŸ Final Outcome

The final **Aurora Member Q&A System**:
- Provides accurate, context-aware answers grounded in real member data.  
- Operates fully on **CPU-based infrastructure** with persistent vector storage.  
- Deploys seamlessly on **Railway**, supporting scalable API queries.  
- Maintains **explainability** through transparent logs and contextual reasoning.  

> ğŸ§  *This architecture reflects a deliberate trade-off â€” prioritizing accuracy, interpretability, and reproducibility over raw speed, resulting in a robust and production-ready RAG system.*

---

## ğŸ—ºï¸ RAG Architecture Flow

```mermaid
graph LR
A[ğŸ’¬ User Question] --> B[ğŸ§­ User Detection<br>(Literal + Fuzzy Matching)]
B --> C[ğŸ” Semantic Retrieval<br>via ChromaDB]
C --> D[ğŸ§© Context Augmentation<br>(Timestamped Messages)]
D --> E[ğŸ¤– LLM Generation<br>(GPT-4o-mini)]
E --> F[âœ… Grounded Answer]

style A fill:#f9f9f9,stroke:#aaa,stroke-width:1px;
style F fill:#e2f7e2,stroke:#6c6,stroke-width:1px;
